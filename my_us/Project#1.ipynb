{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9637649d-7749-4f6c-b2ba-bcb0036483f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Table:\n",
      "\n",
      "              File       Start Date         End Date  Total Observations  Missing Values\n",
      "    ESUSD_5min.csv 2025-05-06 00:05 2025-05-15 23:40                2193               0\n",
      "    NQUSD_5min.csv 2025-05-06 00:05 2025-05-15 23:40                2192               0\n",
      "      QQQ_5min.csv 2025-05-06 09:30 2025-05-15 15:55                 624               0\n",
      "      SPY_5min.csv 2025-05-06 09:30 2025-05-15 15:55                 624               0\n",
      "   USDJPY_5min.csv 2025-05-06 00:00 2025-05-15 23:50                2302               0\n",
      "   ESUSD_15min.csv 2025-04-01 00:00 2025-05-15 23:30                2944               0\n",
      "   NQUSD_15min.csv 2025-04-01 00:00 2025-05-15 23:30                2941               0\n",
      "     QQQ_15min.csv 2025-04-01 09:30 2025-05-15 15:45                 832               0\n",
      "     SPY_15min.csv 2025-04-01 09:30 2025-05-15 15:45                 832               0\n",
      "  USDJPY_15min.csv 2025-04-01 00:00 2025-05-15 23:45                3161               0\n",
      "   ESUSD_30min.csv 2025-04-16 00:00 2025-05-15 23:30                 967               0\n",
      "   NQUSD_30min.csv 2025-04-16 00:00 2025-05-15 23:30                 967               0\n",
      "     QQQ_30min.csv 2025-04-16 09:30 2025-05-15 15:30                 273               0\n",
      "     SPY_30min.csv 2025-04-16 09:30 2025-05-15 15:30                 273               0\n",
      "  USDJPY_30min.csv 2025-04-16 00:00 2025-05-15 23:30                1052               0\n",
      "   ESUSD_60min.csv 2025-02-16 18:00 2025-05-15 23:00                1447               0\n",
      "   NQUSD_60min.csv 2025-02-16 18:00 2025-05-15 23:00                1443               0\n",
      "     QQQ_60min.csv 2025-02-18 09:30 2025-05-15 15:30                 434               0\n",
      "     SPY_60min.csv 2025-02-18 09:30 2025-05-15 15:30                 434               0\n",
      "  USDJPY_60min.csv 2025-02-16 18:00 2025-05-15 23:00                1529               0\n",
      "  ESUSD_240min.csv 2024-11-17 18:00 2025-05-15 20:00                 759               0\n",
      "  NQUSD_240min.csv 2024-11-17 18:00 2025-05-15 20:00                 751               0\n",
      "    QQQ_240min.csv 2024-11-18 11:30 2025-05-15 13:30                 242               0\n",
      "    SPY_240min.csv 2024-11-18 11:30 2025-05-15 13:30                 242               0\n",
      " USDJPY_240min.csv 2024-11-17 18:00 2025-05-15 20:00                 760               0\n",
      " ESUSD_1440min.csv 2024-01-02 00:00 2025-05-15 00:00                 370               0\n",
      " NQUSD_1440min.csv 2024-01-01 00:00 2025-05-15 00:00                 407               0\n",
      "   QQQ_1440min.csv 2024-01-02 00:00 2025-05-15 00:00                 344               0\n",
      "   SPY_1440min.csv 2024-01-02 00:00 2025-05-15 00:00                 344               0\n",
      "USDJPY_1440min.csv 2024-01-01 00:00 2025-05-15 00:00                 377               0\n",
      "\n",
      "Summary saved to: 'data_file_summary.xlsx'\n",
      "Successfully processed 30 out of 30 files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Google Cloud Storage configuration\n",
    "bucket_name = \"adamantine-market-data\"\n",
    "gcs_folder = \"market_data/\"\n",
    "\n",
    "# File list in desired order\n",
    "file_names = [\n",
    "    'ESUSD_5min.csv', 'NQUSD_5min.csv', 'QQQ_5min.csv', 'SPY_5min.csv', 'USDJPY_5min.csv',\n",
    "    'ESUSD_15min.csv', 'NQUSD_15min.csv', 'QQQ_15min.csv', 'SPY_15min.csv', 'USDJPY_15min.csv',\n",
    "    'ESUSD_30min.csv', 'NQUSD_30min.csv', 'QQQ_30min.csv', 'SPY_30min.csv', 'USDJPY_30min.csv',\n",
    "    'ESUSD_60min.csv', 'NQUSD_60min.csv', 'QQQ_60min.csv', 'SPY_60min.csv', 'USDJPY_60min.csv',\n",
    "    'ESUSD_240min.csv', 'NQUSD_240min.csv', 'QQQ_240min.csv', 'SPY_240min.csv', 'USDJPY_240min.csv',\n",
    "    'ESUSD_1440min.csv', 'NQUSD_1440min.csv', 'QQQ_1440min.csv', 'SPY_1440min.csv', 'USDJPY_1440min.csv'\n",
    "]\n",
    "\n",
    "# Create storage client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "summary_data = []\n",
    "processed_files = 0\n",
    "\n",
    "for file in file_names:\n",
    "    blob_path = f\"{gcs_folder}{file}\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if not blob.exists(client):\n",
    "        print(f\"Warning: File '{blob_path}' not found in GCS bucket.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Read CSV from GCS into pandas DataFrame\n",
    "        data = blob.download_as_bytes()\n",
    "        df = pd.read_csv(io.BytesIO(data))\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df = df.dropna(subset=['date'])\n",
    "\n",
    "        start_date = df['date'].min().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        end_date = df['date'].max().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        total_rows = len(df)\n",
    "        missing_values = df.isnull().sum().sum()\n",
    "\n",
    "        summary_data.append({\n",
    "            'File': file,\n",
    "            'Start Date': start_date,\n",
    "            'End Date': end_date,\n",
    "            'Total Observations': total_rows,\n",
    "            'Missing Values': missing_values\n",
    "        })\n",
    "        processed_files += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Check if any files were processed successfully\n",
    "if processed_files == 0:\n",
    "    print(\"\\nNo files were processed successfully. Please check if the files exist in GCS.\")\n",
    "    exit()\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "ordered_columns = ['File', 'Start Date', 'End Date', 'Total Observations', 'Missing Values']\n",
    "summary_df = summary_df[ordered_columns]\n",
    "\n",
    "# Save summary to local disk\n",
    "output_path = \"data_file_summary.xlsx\"\n",
    "summary_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"\\nSummary Table:\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nSummary saved to: '{output_path}'\")\n",
    "print(f\"Successfully processed {processed_files} out of {len(file_names)} files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1711ee1f-4d95-4736-bde9-37fcf5a40cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Timestamp Match Summary:\n",
      "\n",
      "                               Pair  Matched  Only in ES  Only in NQ\n",
      "    ESUSD_5min.csv & NQUSD_5min.csv     2188           5           4\n",
      "  ESUSD_15min.csv & NQUSD_15min.csv     2940           4           1\n",
      "  ESUSD_30min.csv & NQUSD_30min.csv      967           0           0\n",
      "  ESUSD_60min.csv & NQUSD_60min.csv     1443           4           0\n",
      "ESUSD_240min.csv & NQUSD_240min.csv      750           9           1\n",
      "\n",
      " Summary saved at: matched_timestamp_outputs/timestamp_match_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import os\n",
    "\n",
    "# GCS settings\n",
    "bucket_name = \"adamantine-market-data\"\n",
    "gcs_folder = \"market_data/\"\n",
    "\n",
    "# Initialize client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Define exact filename pairs\n",
    "matched_pairs = [\n",
    "    (\"ESUSD_5min.csv\", \"NQUSD_5min.csv\"),\n",
    "    (\"ESUSD_15min.csv\", \"NQUSD_15min.csv\"),\n",
    "    (\"ESUSD_30min.csv\", \"NQUSD_30min.csv\"),\n",
    "    (\"ESUSD_60min.csv\", \"NQUSD_60min.csv\"),\n",
    "    (\"ESUSD_240min.csv\", \"NQUSD_240min.csv\")\n",
    "]\n",
    "\n",
    "# Output folder\n",
    "output_dir = \"matched_timestamp_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Summary list\n",
    "summary = []\n",
    "\n",
    "# Process each pair\n",
    "for es_file, nq_file in matched_pairs:\n",
    "    try:\n",
    "        # Load from GCS\n",
    "        es_blob = bucket.blob(f\"{gcs_folder}{es_file}\")\n",
    "        nq_blob = bucket.blob(f\"{gcs_folder}{nq_file}\")\n",
    "        es_df = pd.read_csv(io.BytesIO(es_blob.download_as_bytes()))\n",
    "        nq_df = pd.read_csv(io.BytesIO(nq_blob.download_as_bytes()))\n",
    "\n",
    "        # Parse dates\n",
    "        es_df['date'] = pd.to_datetime(es_df['date'], errors='coerce')\n",
    "        nq_df['date'] = pd.to_datetime(nq_df['date'], errors='coerce')\n",
    "        es_df = es_df.dropna(subset=['date'])\n",
    "        nq_df = nq_df.dropna(subset=['date'])\n",
    "\n",
    "        # Match timestamps\n",
    "        matched_df = pd.merge(es_df[['date']], nq_df[['date']], on='date', how='inner')\n",
    "        only_in_es = es_df[~es_df['date'].isin(nq_df['date'])]\n",
    "        only_in_nq = nq_df[~nq_df['date'].isin(es_df['date'])]\n",
    "\n",
    "        # Save matched dates\n",
    "        out_file = f\"matched_{es_file.replace('.csv','')}_{nq_file.replace('.csv','')}.xlsx\"\n",
    "        matched_df.to_excel(os.path.join(output_dir, out_file), index=False)\n",
    "\n",
    "        # Record summary\n",
    "        summary.append({\n",
    "            \"Pair\": f\"{es_file} & {nq_file}\",\n",
    "            \"Matched\": len(matched_df),\n",
    "            \"Only in ES\": len(only_in_es),\n",
    "            \"Only in NQ\": len(only_in_nq)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        summary.append({\n",
    "            \"Pair\": f\"{es_file} & {nq_file}\",\n",
    "            \"Matched\": \"Error\",\n",
    "            \"Only in ES\": \"Error\",\n",
    "            \"Only in NQ\": str(e)\n",
    "        })\n",
    "\n",
    "# Final summary table\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_path = os.path.join(output_dir, \"timestamp_match_summary.xlsx\")\n",
    "#summary_df.to_excel(summary_path, index=False)\n",
    "\n",
    "# Print\n",
    "print(\"\\n Timestamp Match Summary:\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\n Summary saved at: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d77bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Descriptive Statistics:\n",
      "\n",
      "                  File Metric          Mean       Std Dev      Min        Max  Skewness  Kurtosis\n",
      "  ESUSD_ESUSD_5min.csv   open   5779.349179    110.258131  5600.25    5942.50  0.028773 -1.694174\n",
      "  ESUSD_ESUSD_5min.csv   high   5781.519494    110.023273  5608.75    5944.50  0.030029 -1.697802\n",
      "  ESUSD_ESUSD_5min.csv    low   5777.115253    110.593801  5597.75    5940.25  0.027375 -1.690981\n",
      "  ESUSD_ESUSD_5min.csv  close   5779.392841    110.324413  5601.75    5942.75  0.028019 -1.694756\n",
      "  ESUSD_ESUSD_5min.csv volume   3481.369813   6918.328326     0.00  106671.00  5.191309 47.767522\n",
      "  NQUSD_NQUSD_5min.csv   open  20622.662568    578.866960 19703.75   21523.00  0.106394 -1.636983\n",
      "  NQUSD_NQUSD_5min.csv   high  20632.346943    577.872140 19731.00   21529.75  0.107561 -1.640202\n",
      "  NQUSD_NQUSD_5min.csv    low  20612.782276    580.262929 19678.75   21512.25  0.103955 -1.633860\n",
      "  NQUSD_NQUSD_5min.csv  close  20623.024407    579.209109 19703.75   21523.50  0.104922 -1.637735\n",
      "  NQUSD_NQUSD_5min.csv volume   1706.156934   2507.740364     0.00   27434.00  2.929174 13.176396\n",
      " ESUSD_ESUSD_15min.csv   open   5511.101987    237.523464  4861.00    5941.50 -0.404123 -0.128618\n",
      " ESUSD_ESUSD_15min.csv   high   5518.392238    233.838737  4878.75    5944.50 -0.382128 -0.152308\n",
      " ESUSD_ESUSD_15min.csv    low   5504.113876    240.900251  4840.00    5936.50 -0.420524 -0.112391\n",
      " ESUSD_ESUSD_15min.csv  close   5511.128312    237.650101  4861.25    5941.25 -0.401745 -0.135664\n",
      " ESUSD_ESUSD_15min.csv volume  16549.396060  25287.498827     0.00  267409.00  3.002883 14.156929\n",
      " NQUSD_NQUSD_15min.csv   open  19314.826165   1077.696669 16552.50   21519.75 -0.076024 -0.227145\n",
      " NQUSD_NQUSD_15min.csv   high  19343.760031   1065.578523 16596.75   21529.75 -0.058852 -0.231988\n",
      " NQUSD_NQUSD_15min.csv    low  19287.174600   1088.507611 16469.00   21496.00 -0.088712 -0.226109\n",
      " NQUSD_NQUSD_15min.csv  close  19315.406239   1077.790348 16543.00   21520.25 -0.072086 -0.234022\n",
      " NQUSD_NQUSD_15min.csv volume   4598.754845   7278.134629     0.00   59204.00  2.409128  6.796643\n",
      " ESUSD_ESUSD_30min.csv   open   5594.734230    199.249730  5131.25    5941.50 -0.180825 -0.683727\n",
      " ESUSD_ESUSD_30min.csv   high   5602.096949    197.728599  5143.00    5944.50 -0.184844 -0.671511\n",
      " ESUSD_ESUSD_30min.csv    low   5588.071613    200.537377  5127.25    5934.75 -0.174040 -0.695526\n",
      " ESUSD_ESUSD_30min.csv  close   5595.117632    199.362435  5131.25    5941.25 -0.181668 -0.680830\n",
      " ESUSD_ESUSD_30min.csv volume  25697.883144  38093.497659     0.00  287432.00  2.135029  5.507760\n",
      " NQUSD_NQUSD_30min.csv   open  19729.335832    954.387636 17708.00   21519.75 -0.014328 -0.705139\n",
      " NQUSD_NQUSD_30min.csv   high  19760.324716    949.105346 17755.75   21529.75 -0.015999 -0.697793\n",
      " NQUSD_NQUSD_30min.csv    low  19701.240951    959.190204 17700.00   21485.25 -0.011708 -0.713066\n",
      " NQUSD_NQUSD_30min.csv  close  19731.477249    955.336443 17709.00   21520.25 -0.015493 -0.704008\n",
      " NQUSD_NQUSD_30min.csv volume  11588.056877  14474.872328     0.00   88320.00  1.754378  2.750960\n",
      " ESUSD_ESUSD_60min.csv   open   5666.180546    266.011041  4863.00    6165.25 -0.349389  0.142495\n",
      " ESUSD_ESUSD_60min.csv   high   5677.854872    260.571782  4891.75    6166.50 -0.318022  0.084414\n",
      " ESUSD_ESUSD_60min.csv    low   5654.092605    271.554379  4840.00    6155.50 -0.388248  0.211376\n",
      " ESUSD_ESUSD_60min.csv  close   5665.910159    265.566654  4863.00    6165.25 -0.345122  0.132411\n",
      " ESUSD_ESUSD_60min.csv volume  70747.658604  97968.287242     0.00  561123.00  1.838947  3.322345\n",
      " NQUSD_NQUSD_60min.csv   open  19895.321206   1179.941207 16552.50   22314.75  0.000732  0.002696\n",
      " NQUSD_NQUSD_60min.csv   high  19944.606930   1161.154376 16695.25   22319.75  0.028061 -0.040059\n",
      " NQUSD_NQUSD_60min.csv    low  19843.835412   1197.945252 16469.00   22296.50 -0.029395  0.035792\n",
      " NQUSD_NQUSD_60min.csv  close  19894.037422   1177.797239 16543.00   22314.00  0.007321 -0.013243\n",
      " NQUSD_NQUSD_60min.csv volume  22436.311157  30703.605740     0.00  160297.00  1.669768  1.982980\n",
      "ESUSD_ESUSD_240min.csv   open   5842.397563    269.564795  4880.50    6157.25 -1.111308  0.746403\n",
      "ESUSD_ESUSD_240min.csv   high   5860.007576    258.971081  4935.50    6166.50 -1.049603  0.475984\n",
      "ESUSD_ESUSD_240min.csv    low   5823.026680    278.819039  4840.00    6151.25 -1.140078  0.875052\n",
      "ESUSD_ESUSD_240min.csv  close   5842.514163    268.854556  4880.50    6157.00 -1.089978  0.650604\n",
      "ESUSD_ESUSD_240min.csv volume 243588.113307 292432.932118     0.00 1460952.00  1.423137  1.448459\n",
      "NQUSD_NQUSD_240min.csv   open  20635.241678   1187.660955 16577.25   22298.25 -0.872433  0.202379\n",
      "NQUSD_NQUSD_240min.csv   high  20713.721838   1151.456300 16825.50   22319.75 -0.813356 -0.017031\n",
      "NQUSD_NQUSD_240min.csv    low  20547.520639   1217.136649 16469.00   22263.00 -0.895455  0.278558\n",
      "NQUSD_NQUSD_240min.csv  close  20635.542943   1184.647014 16681.50   22297.50 -0.850283  0.105383\n",
      "NQUSD_NQUSD_240min.csv volume  82213.414115  94775.180910     0.00  413405.00  1.284408  0.581011\n"
     ]
    }
   ],
   "source": [
    "#Descriptive Statistics\n",
    "import pandas as pd\n",
    "import io\n",
    "from scipy.stats import skew, kurtosis\n",
    "from google.cloud import storage\n",
    "\n",
    "# Define GCS connection\n",
    "bucket_name = \"adamantine-market-data\"\n",
    "gcs_folder = \"market_data/\"\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Exact file pairs\n",
    "matched_pairs = [\n",
    "    (\"ESUSD_5min.csv\", \"NQUSD_5min.csv\"),\n",
    "    (\"ESUSD_15min.csv\", \"NQUSD_15min.csv\"),\n",
    "    (\"ESUSD_30min.csv\", \"NQUSD_30min.csv\"),\n",
    "    (\"ESUSD_60min.csv\", \"NQUSD_60min.csv\"),\n",
    "    (\"ESUSD_240min.csv\", \"NQUSD_240min.csv\")\n",
    "]\n",
    "\n",
    "# Collect stats\n",
    "all_stats = []\n",
    "\n",
    "for es_file, nq_file in matched_pairs:\n",
    "    for label, file in zip([\"ESUSD\", \"NQUSD\"], [es_file, nq_file]):\n",
    "        try:\n",
    "            blob = bucket.blob(f\"{gcs_folder}{file}\")\n",
    "            df = pd.read_csv(io.BytesIO(blob.download_as_bytes()))\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            df = df.dropna(subset=['date'])\n",
    "\n",
    "            for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "                if col in df.columns:\n",
    "                    series = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "                    all_stats.append({\n",
    "                        \"File\": f\"{label}_{file}\",\n",
    "                        \"Metric\": col,\n",
    "                        \"Mean\": series.mean(),\n",
    "                        \"Std Dev\": series.std(),\n",
    "                        \"Min\": series.min(),\n",
    "                        \"Max\": series.max(),\n",
    "                        \"Skewness\": skew(series),\n",
    "                        \"Kurtosis\": kurtosis(series)\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            all_stats.append({\n",
    "                \"File\": f\"{label}_{file}\",\n",
    "                \"Metric\": \"ERROR\",\n",
    "                \"Mean\": \"N/A\",\n",
    "                \"Std Dev\": \"N/A\",\n",
    "                \"Min\": \"N/A\",\n",
    "                \"Max\": \"N/A\",\n",
    "                \"Skewness\": \"N/A\",\n",
    "                \"Kurtosis\": str(e)\n",
    "            })\n",
    "\n",
    "# Final summary DataFrame\n",
    "stats_df = pd.DataFrame(all_stats)\n",
    "#stats_df.to_excel(\"descriptive_stats_summary.xlsx\", index=False)\n",
    "\n",
    "# Save and print\n",
    "print(\"\\nüìä Descriptive Statistics:\\n\")\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39302f61",
   "metadata": {},
   "source": [
    "#### Why focus on 15-minute, 30-minute, and 1-hour prediction horizons?\n",
    "| Key Trait              | Ideal Condition                     | Why It Matters                       |\n",
    "| ---------------------- | ----------------------------------- | ------------------------------------ |\n",
    "| **Skewness**           | Close to 0                          | Reduces bias in prediction           |\n",
    "| **Kurtosis**           | Close to 0‚Äì1                        | Limits outlier influence             |\n",
    "| **Volume stability**   | Moderate SD and lower kurtosis/skew | Reduces false spikes, smoother model |\n",
    "| **Standard Deviation** | Not excessively high                | Reflects better signal quality       |\n",
    "\n",
    "üîò 5-Minute: ‚ùå Too noisy ‚Äî avoid.\n",
    "Price Std Dev: 110.32 (ES), 579.21 (NQ) ‚ùó | Skewness: +0.028 (ES), +0.105 (NQ) ‚úî | Volume skewness: 5.19 (ES), 2.93 (NQ) ‚ùó\n",
    "Kurtosis: Price ‚Äì1.695 (ES), ‚Äì1.638 (NQ) ‚úî | Volume kurtosis: 47.77 (ES), 13.18 (NQ) ‚ùó\n",
    "Interpretation: Stable prices but high-frequency noise and extreme volume outliers make this timeframe unsuitable for reliable spread modeling.\n",
    "\n",
    "üîò 15-Minute: ‚úÖ Best all-around choice for signal generation.\n",
    "Price Std Dev: 237.65 (ES), 1077.79 (NQ) ‚úî | Skewness: ‚Äì0.402 (ES), ‚Äì0.072 (NQ) ‚úî | Volume skewness: 3.00 (ES), 2.41 (NQ) ‚ö†\n",
    "Kurtosis: Price ‚Äì0.136 (ES), ‚Äì0.234 (NQ) ‚úî | Volume kurtosis: 14.16 (ES), 6.80 (NQ) ‚ö†\n",
    "Interpretation: Smooth and symmetric price dynamics offer a strong foundation for Z-score prediction; volume extremes are secondary.\n",
    "\n",
    "üîò 30-Minute: ‚úÖ Strong supporting timeframe.\n",
    "Price Std Dev: 199.36 (ES), 955.34 (NQ) ‚úî | Skewness: ‚Äì0.182 (ES), ‚Äì0.015 (NQ) ‚úî | Volume skewness: 2.14 (ES), 1.75 (NQ) ‚úî\n",
    "Kurtosis: Price ‚Äì0.681 (ES), ‚Äì0.704 (NQ) ‚úî | Volume kurtosis: 5.51 (ES), 2.75 (NQ) ‚úî\n",
    "Interpretation: Well-behaved price distributions and lower noise make this timeframe ideal for validating Z-score-based signals.\n",
    "\n",
    "üîò 60-Minute: ‚úÖ Ideal for strategic overlays, not quick exits.\n",
    "Price Std Dev: 265.57 (ES), 1177.80 (NQ) ‚úî | Skewness: ‚Äì0.345 (ES), +0.007 (NQ) ‚úî | Volume skewness: 1.84 (ES), 1.67 (NQ) ‚úî\n",
    "Kurtosis: Price +0.132 (ES), ‚Äì0.013 (NQ) ‚úî | Volume kurtosis: 3.32 (ES), 1.98 (NQ) ‚úî\n",
    "Interpretation: Stable, near-normal distribution supports macro-level regime detection; ideal for risk filters or directional overlays.\n",
    "\n",
    "üîò 240-Minute: ‚ùå Use only for macro filters ‚Äî avoid for prediction.\n",
    "Price Std Dev: 268.85 (ES), N/A (NQ close missing) ‚ùó | Skewness: ‚Äì1.090 (ES), ‚Äì0.850 (NQ) ‚ùó | Volume skewness: 1.42 (ES), 1.28 (NQ) ‚úî\n",
    "Kurtosis: Price +0.651 (ES), +0.105 (NQ) ‚úî | Volume kurtosis: 1.45 (ES), 0.58 (NQ) ‚úî\n",
    "Interpretation: Negatively skewed and sluggish behavior reflects directional bias; not suitable for short-term modeling or timely exits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99cee6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Correlation Summary:\n",
      "\n",
      "                               Pair  Correlation  Observations\n",
      "    ESUSD_5min.csv & NQUSD_5min.csv     0.994750          2188\n",
      "  ESUSD_15min.csv & NQUSD_15min.csv     0.988819          2940\n",
      "  ESUSD_30min.csv & NQUSD_30min.csv     0.997655           967\n",
      "  ESUSD_60min.csv & NQUSD_60min.csv     0.984807          1443\n",
      "ESUSD_240min.csv & NQUSD_240min.csv     0.981284           750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from google.cloud import storage\n",
    "\n",
    "# ---------------------------\n",
    "# GCS Setup\n",
    "# ---------------------------\n",
    "bucket_name = \"adamantine-market-data\"\n",
    "gcs_folder = \"market_data/\"\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# ---------------------------\n",
    "# Define exact matched pairs\n",
    "# ---------------------------\n",
    "matched_pairs = [\n",
    "    (\"ESUSD_5min.csv\", \"NQUSD_5min.csv\"),\n",
    "    (\"ESUSD_15min.csv\", \"NQUSD_15min.csv\"),\n",
    "    (\"ESUSD_30min.csv\", \"NQUSD_30min.csv\"),\n",
    "    (\"ESUSD_60min.csv\", \"NQUSD_60min.csv\"),\n",
    "    (\"ESUSD_240min.csv\", \"NQUSD_240min.csv\")\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# Analyze correlation\n",
    "# ---------------------------\n",
    "correlation_stats = []\n",
    "\n",
    "for es_file, nq_file in matched_pairs:\n",
    "    try:\n",
    "        # Read ESUSD file from GCS\n",
    "        es_blob = bucket.blob(f\"{gcs_folder}{es_file}\")\n",
    "        es_data = es_blob.download_as_bytes()\n",
    "        es_df = pd.read_csv(io.BytesIO(es_data))\n",
    "        es_df['date'] = pd.to_datetime(es_df['date'], errors='coerce')\n",
    "        es_df = es_df.dropna(subset=['date', 'close'])\n",
    "\n",
    "        # Read NQUSD file from GCS\n",
    "        nq_blob = bucket.blob(f\"{gcs_folder}{nq_file}\")\n",
    "        nq_data = nq_blob.download_as_bytes()\n",
    "        nq_df = pd.read_csv(io.BytesIO(nq_data))\n",
    "        nq_df['date'] = pd.to_datetime(nq_df['date'], errors='coerce')\n",
    "        nq_df = nq_df.dropna(subset=['date', 'close'])\n",
    "\n",
    "        # Merge on date and calculate correlation\n",
    "        merged_df = pd.merge(\n",
    "            es_df[['date', 'close']], \n",
    "            nq_df[['date', 'close']], \n",
    "            on='date', \n",
    "            suffixes=('_es', '_nq')\n",
    "        ).dropna()\n",
    "\n",
    "        # Pearson correlation between ES and NQ closing prices\n",
    "        correlation = merged_df['close_es'].corr(merged_df['close_nq'])\n",
    "\n",
    "        correlation_stats.append({\n",
    "            \"Pair\": f\"{es_file} & {nq_file}\",\n",
    "            \"Correlation\": correlation,\n",
    "            \"Observations\": len(merged_df)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        correlation_stats.append({\n",
    "            \"Pair\": f\"{es_file} & {nq_file}\",\n",
    "            \"Correlation\": \"Error\",\n",
    "            \"Observations\": 0,\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "# ---------------------------\n",
    "# Print or save results\n",
    "# ---------------------------\n",
    "correlation_df = pd.DataFrame(correlation_stats)\n",
    "print(\"\\nüìà Correlation Summary:\\n\")\n",
    "print(correlation_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
